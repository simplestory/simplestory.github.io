---
layout:     post
title:      SVM支持向量机
subtitle:   
date:       2018-8-16
author:     Simplestory
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---

> SVM是机器学习里面一个非常重要的模型，它是由统计学习理论之父弗拉基米尔·瓦普尼克提出的

**本文以SVM二分类为例**

## 推导过程

给定训练数据集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}, y_i \in \{-1, +1\}$，分类器最基本的想法就是基于训练集D在样本空间中找到一个划分空间，将不同类别的样本分开。划分超平面可用如下的线性方程描述：

$$ w^Tx+b=0$$

其中$w=(w_1;w_2;...;w_d)$为法向量，决定了超平面的方向，b为偏移项，决定了超平面与原点之间的距离，可知超平面有w和d决定。则有空间中任意点到超平面(w,b)的距离为：

$$\gamma = \frac{|w^Tx+b|}{||w||}$$

假设超平面(w,b)将样本完全分类正确，即对于$(x_i, y_i) \in D$，若$y_i=+1$则有$w^Tx+b > 0$; 若$y_i=-1$，则有$w^Tx+b < 0$，故有：

$$
\begin{cases}
w^Tx+b \ge +1, y_i = +1  \\
w^Tx+b \le -1, y_I = -1
\end{cases}
$$(1)

上式可以合并为：$y_i(w^Tx+b) \ge 1, i=1,2,...,m$

在距离超平面最近的几个训练样本点使(1)式等号成立，这些样本点被称为“支持向量”，两个异类支持向量到超平面的距离之和为：

$$\gamma = \frac{2}{||w||}$$

它被称为“间隔”

SVM则是找到具有最大间隔的划分超平面，即满足下式：

$$
\begin{aligned}
\min_{w,b} & \frac{1}{2}||w||^2  \\
s.t. & \ y_i(w_ix+b) \ge 1, i=1,2,...,m
\end{aligned}
$$(2)

上式即为SVM基本型

对(2)使用拉格朗日乘数法可得下式：

$$L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^m \alpha_i(1-y_i(w^Tx+b))$$(3)

其中$\alpha = (\alpha_1;\alpha_2;...;\alpha_m)$，令$L(w,b,\alpha)$对w，b的偏导数为0可得：

$$
\begin{cases}
w = \sum_{i=1}^m \alpha_iy_ix_i  \\
0 = \sum_{i=1}^m \alpha_iy_i
\end{cases}
$$(4)

将(4)式代入(3)，可得：

$$
\begin{aligned}
\max_\alpha & \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_jy_iy_jx_i^Tx_j  \\
s.t. & \sum_{i=1}^m \alpha_iy_i=0,  \\
& \alpha_i \ge 0, i=1,2,...,m
\end{aligned}
$$(5)

解出$\alpha$后，求出w，b即可得到模型如下：

$$
f(x)=w^Tx+b=\sum_{i=1}^m \alpha_iy_ix_i^Tx+b
$$

求解(5)式通常采用的是SMO算法得出$\alpha$的值，再由(6)式计算b的值：

$$
b = \frac{1}{|s|}\sum_{s \in S}(\frac{1}{y_c}-\sum_{i \in s}\alpha_iy_ix_i^Tx_s)
$$(6)

## 核技巧

在前面的讨论中，我们假设了数据是线性可分的，但现实数据大多数情况下并非线性可分。为此，我们可以考虑对原始数据做一个映射，使其线性可分，之后再交由SVM模型训练。具体如下：

假设映射函数为$\phi(x)$，则有：$f(x)=w^T\phi(x)+b$

$$
\begin{aligned}
\min_{w,b} & \frac{1}{2}||w||^2  \\
s.t. & \ y_i(w^T\phi(x_i)+b) \ge 1, i=1,2,...,m
\end{aligned}
$$

其对偶问题为：

$$
\begin{aligned}
\max_\alpha & \sum_{i=1}^m \alpha_i-\frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)  \\
s.t. & \ \sum_{i=1}^m \alpha_iy_i=0  \\
& \alpha_i \ge 0, i=1,2,...,m
\end{aligned}
$$

对于以上式子出现的$\phi(x_i)^T\phi(x_j)$可用一个函数来替代$k(x_i,x_j)$，即为核函数

$$k(x_i,x_j)=\langle \phi(x_i),\phi(x_j) \rangle=\phi(x_i)^T\phi(x_j)$$

故有：

$$
\begin{aligned}
\max_\alpha & \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i-1}^m \sum_{j=1}^m \alpha_i\alpha_jy_iy_jk(x_i,x_j)  \\
s.t. & \ \sum_{i=1}^m \alpha_iy_i = 0,  \\
& \alpha_i \ge 0, i=1,2,..,m
\end{aligned}
$$

求解后有：

$$
f(x)=w^T\phi(x)+b=\sum_{i=1}^m \alpha_iy_i\phi(x_i)^T\phi(x_j)+b
=\sum_{i=1}^m \alpha_iy_ik(x,x_i)+b
$$

常用的核函数：

名称|表达式|参数
-----|-----|----
线性核|$k(x_i,x_j)=x_i^Tx_j$|
多项式核|$k(x_i,x_j)=(x_i^Tx_j)^d$|$d \ge 1$，为多项式次数
高斯核|$k(x_i,x_j)=exp(-\frac{\|x_i-x_j\|^2}{2\sigma^2})$|$\sigma > 0$为高斯核的带宽
拉普拉斯核|$k(x_i,x_j)=exp(-\frac{\|x_i-x_j\|}{\sigma})$|$\sigma > 0$
Sigmoid核|$k(x_i,x_j)=tanh(\beta x_i^Tx_j+\theta)$|tanh为双曲正切函数，$\beta > 0, \theta > 0$

通常的选择：

线性核一般用于线性可分的数据或文本数据

高斯核一般用于线性不可分或数据情况不明的时候

## SVM的正则化

为了防止过拟合，我们通常采用软间隔支持向量机，即允许有某些样本在间隔内，则模型方程为：

$$
\min_{w,b} \frac{1}{2}\|w\|^2+C\sum_{i=1}^m l_{0/1}(y_i(w^Tx_i+b)-1)
$$(7)

其中C>0是一个常数，$l_{0/1}$为0/1损失函数：

$$
l_{0/1}(z)=\begin{cases}
1, \text{if}z \le 0;  \\
0, \text{otherwise}
\end{cases}
$$

通常用其它函数替换$l_{0/1}$，如下：

$$
\begin{cases}
\text{hinge损失:} l_{hinge}(z)=\max(0,1-z);  \\
\text{指数损失：} l_{exp}(z)=exp(-z);  \\
\text{对率损失：} l_{log}(z)=log(1+exp(-z))
\end{cases}
$$

参考(7)式，更一般的形式为：

$$\min_f \Omega(f)+C\sum_{i=1}^m l(f(x_i),y_i)$$

其中$\Omega(f)$为结构风险，用于描述模型f的某些性质（$\Omega(f)$又可称为正则化项，常用的有$L_0,L_1,L_2$正则化），第二项$\sum_{i=1}^m l(f(x_i),y_i)$为经验风险，由于描述模型与训练数据的契合程度，C由于两者进行折中。

## Scikit-learn应用

具体API参考Skicit-learn官网：

[sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)

[sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)

## 致谢

本文参考自：
周志华的西瓜书，李航的统计学习方法