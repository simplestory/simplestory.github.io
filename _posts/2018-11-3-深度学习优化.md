---
layout:     post
title:      深度学习优化
subtitle:   
date:       2018-11-03
author:     Simplestory
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Deep Learning
---

>最优化是一个十分困难的问题，通常要精心设计一个目标函数及相关约束，同时还要确保所要优化的问题为凸函数，但现实中的问题大多为非凸函数，针对非凸优化问题，这里有一些基本的优化方法：SGD、Momentum、AdaGrad、RMSProp及Adam等

## 随机梯度下降(Stochastic Gradient Descent, SGD)

---

// 随机梯度下降

给定数据集$X=\{x^{(1)},x^{(2)},...,x^{(n)}\}$, 数据集标记$Y=\{y^{(1)}, y^{(2)},...,y^{(n)}\}$

学习器$f(x;w)$, 学习率（步长）$\alpha$

For 迭代足够多次

{

随机选择数据： $\{x^{(j)}, y^{(j)}\}$

计算损失梯度： $\nabla\mathbf{w}=\frac{\partial{L(y^{(j)},f(x^{(j)};w))}}{\partial{w}}$

修改权重： $\mathbf{w_i}=\mathbf{w_i}-\alpha\nabla\mathbf{w}$

}

---

以上是从数据集中随机选择一条数据进行梯度下降，故在训练过程中会出现很强的随机现象，为了避免这种随机性，可以选择多条数据进行梯度计算，之后再取平均值。

对于学习率，在上面的算法中，学习率一直保持不变，这在训练后期会造成算法在最优解附近振荡的现象，为了消除或缓解这种情况，我们可以在靠近最优解周围时尽可能地减小学习率

综合以上可得以下梯度下降算法

---

// 学习率衰减最小批量梯度下降

给定数据集$X=\{x^{(1)},x^{(2)},...,x^{(n)}\}$, 数据集标记$Y=\{y^{(1)}, y^{(2)},...,y^{(n)}\}$

随机采样$m$条数据,训练周期$k$，学习率衰减最低值$b$,学习器$f(x;w)$

初始学习率（步长）$\alpha_0$

For $i <= k$

{

随机采样$m$条数据： $\{(x^{(i)},y^{(i)}),...,(x^{(m)},y^{(m)})\}$

计算采样数据平均损失值梯度： $\nabla \mathbf{w} = \frac{1}{m}\sum^m_{j=1}\frac{\partial{L(y^{(j)},f(x^{(j)};w)})}{\partial{\mathbf{w}}}$

计算衰减学习率： $a_i=(i-\frac{i}{k})a_0+\frac{i}{k}b$

修改网络权重： $\mathbf{w}_i=\mathbf{w}_i-\alpha\nabla\mathbf{w}$

}

---

## 动量学习法(Momentum)

上面的算法虽然解决了在最优解附近振荡以及随机性的问题，但却不能保证算法找到的解为全局最优解，算法随后有可能找到的是局部最优解，即为鞍点。

为了跳出鞍点，可以借助物理上的概念——动量，冲出鞍点，故有以下动量随机梯度下降法

---

// 动量学习法

给定数据集$X=\{x^{(1)},x^{(2)},...,x^{(n)}\}$, 数据集标记$Y=\{y^{(1)}, y^{(2)},...,y^{(n)}\}$

初始速度$v$,随机采样数据大小$m$，训练周期$k$

学习器$f(x;w)$,初始学习率$\alpha$,初始动量参数$\beta$

For $i <= k$

{

随机采样$m$条数据： $\{(x^{(i)},y^{(i)}),...,(x^{(m)},y^{(m)})\}$

计算采样数据平均损失值梯度： $\nabla \mathbf{w} = \frac{1}{m}\sum^m_{j=1}\frac{\partial{L(y^{(j)},f(x^{(j)};w)})}{\partial{\mathbf{w}}}$

更新速度： $v=\beta v-\alpha\nabla\mathbf{w}$

更新参数： $\mathbf{w}=\mathbf{w}+v$

}

---

在实践中，常用的$\beta$取值可为0.5，0.9或0.99，根据具体问题进行调试，但对于$\beta$的调整，并没有$\alpha$的调整重要，因此不太需要作为超参数进行选择，一般取值适当即可

## AdaGrad和RMSProp

以上的算法都使用一个全局学习率，所有的参数都是统一步伐整齐向前，但网络中的参数可能不会同时有一致的下降梯度和方向，以下算法针对每个参数单独配置学习率

### AdaGrad

该算法其实就是将每一维各自的历史梯度的平方叠加起来，然后在更新的时候除以该历史梯度即可（使用平方的原因是去除梯度的符号，只对梯度的量进行累加）

$$cache_i = cache_i+(\nabla\mathbf{w}_i)^2$$

更新参数：

$$\mathbf{w}_i=\mathbf{w}_i-\frac{\alpha}{\sqrt{cache_i}+\delta}\nabla\mathbf{w}_i$$

其中$\delta=10^{-7}$, 防止除零导致数值溢出

AdaGrad使得参数在累积的梯度量较小时(<1)，放大学习率，使得网络的训练更加快速，在梯度的累积量较大时(>1)，缩小学习率，延缓网络训练

但AdaFrad随着epoch的增长，学习率降低得很快，即该算法很容易过分降低学习率

### RMSProp

为了解决AdaGrad学习率衰减过快的问题，RMSProp算法引入了衰减因子，在进行梯度累积时会对“过去”和“现在”做一个权衡，通过$\beta$来调节衰减量，常用的取值有0.9或0.5

$$cache_i=\beta cache_i+(1-\beta)(\nabla \mathbf{w}_i)^2$$

在参数更新阶段，和AdaGrad相同

$$\mathbf{w}_i=\mathbf{w}_i-\frac{\alpha}{\sqrt{cache_i}+\delta}\nabla\mathbf{w}_i$$

## Adam

该算法是Momentum+RMSProp的微调版本，默认情况下，推荐使用这种优化方法

在开始时梯度会非常小，$r$和$v$经常会接近于0，因此我们还需要对$r$和$v$进行调整

$$
\begin{aligned}
    vb=\frac{v}{1-\beta^t_1} \\
    rb=\frac{r}{1-\beta^t_2}
\end{aligned}
$$

其中$t$表示训练次数，因此该算法仅在训练的前几轮中根据衰减因子来放大各自值，很快$vb$,$rb$会衰减为$v$，$r$

---

// Adam

给定数据集$X=\{x^{(1)},x^{(2)},...,x^{(n)}\}$, 数据集标记$Y=\{y^{(1)}, y^{(2)},...,y^{(n)}\}$

初始速度$v$,随机采样数据大小$m$，训练周期$k$

学习器$f(x;w)$,初始学习率$\alpha$,初始动量参数$\beta_1$

学习率衰减参数$\beta_2$, $\delta=10^{-7}$

For $i <= k$

随机采样$m$条数据： $\{(x^{(i)},y^{(i)}),...,(x^{(m)},y^{(m)})\}$

计算当前采样数据梯度： $g = \frac{1}{m}\sum^m_{j=1}\frac{\partial{L(y^{(j)},f(x^{(j)};w)})}{\partial{\mathbf{w}}}$

更新当前速度： $v=\beta_1 v+(1-\beta_1)g$

更新当前学习率： $r=\beta_2 r+(1-\beta_2)g^2$

更新训练次数： $t=t+1$

$$
\begin{aligned}
    vb=\frac{v}{1-\beta^t_1} \\
    rb=\frac{r}{1-\beta^t_2}
\end{aligned}
$$

更新新参数：

$$w=w-\frac{\alpha}{\sqrt{rb}+\delta}vb$$


