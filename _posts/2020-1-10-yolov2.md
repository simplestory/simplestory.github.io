---
layout:     post
title:      "YoLo_v2 and YoLo9000"
subtitle:   "Better, Faster, Stronger"
date:       2020-01-11
author:     "Simplestory"
header-style: text
catalog: False
mathjax: true
tags:
    - Deep Learning
---

> yolov1算法发布后，引起了大众的强烈反应，有许多研究员将该算法与Fast Rcnn做了比较，yolov1在目标定位上出现了较多的偏差，同时有着较低的召回率。于是作者在几乎同年的时间段里就推出了yolov2以及用于超多类别检测的yolo9000，这算法的迭代速度令人一脸茫然（是真滴NB）。

作者主要围绕着在保持模型精度的前提下提高模型的召回率和降低目标定位偏差这几点来提升算法模型，同时提出了共同训练的方法来实现超多类别检测算法yolo9000。

## Better

作者采用了许多技巧来简化模型的同时提升准确率。
首先是批归一化（Batch Normalization），它可以显著提高模型的收敛性，同时无需其它形式的正则化，像yolov1里面的Dropout层就可以去掉了。作者在所有卷积层后面都添加了BN层，模型$mAP$值提升了至少$2\%$。
yolov1在训练分类网络是使用的输入图像分辨率是$224\times 224$，在实际检测中用到的图像分辨率为$448\times 448$，而yolov2在微调训练分类网络是直接使用了$448\times 448$的大小。这波操作给了模型适应分辨率变化的时间。提高分辨率后，模型$mAP$值提升了大约$4\%$。

众所周知，yolov1并没有使用anchor boxes那一套，而是通过全连接层直接预测出了边界框，但作者发现预测框的偏置量比预测框的坐标要来的简单，并且可以简化网络。所以在yolov2，移除了全连接层并使用anchor boxes来预测边界框。其次，消除了一个池化层，让网络的卷积层输出具有更高的分辨率，同时使用$416\times 416$的图像输入而非$448\times 448$。这里采用$416\times 416$是因为作者想让最后的特征图是一个奇数（下采样32倍，最后输出的特征图大小为$13\times 13$），这样在特征图上就有一个中心点。对于一个物体，特别是大物体，一般都会出现在图像的中心，所以在那里有一个中心点来预测边界框的效果会比周围四个点来预测好一点。当采用anchor boxes机制后，模型的$mAP$值并没有明显的提升，甚至有些许下降，但召回率却有了一定量的提升，总之还是有提升空间的。

在引入anchor boxes机制后，作者面临了两个问题：

- boxes的尺寸。以往的boxes尺寸是手动选择的，之后网络再进行优化。选择一个好的尺寸对模型检测效果的提升是十分巨大的。
- 模型的不稳定性。大部分的不稳定性来自于预测边界框的坐标$(x,y)$。

对于boxes尺寸的选择问题，作者使用了k-means聚类算法来选取先验框的尺寸（讲道理，这波操作骚到不行）。引入anchor boxes的目标是让检测框获得更高的IOU值，而通常的K均值聚类算法使用的距离公式为欧拉公式，这并不符合要求，所以作者更改了距离公式，使用IOU来衡量距离：

$$
d(box, centroid) = 1-IOU(box, centroid)
$$

由于卷积神经网络具有平移不变性，且anchor boxes的位置被每个栅格固定，因此只需通过k-means计算出先验框的长宽即可。通过实验，作者最后选择的k值为5。

针对于模型的稳定性问题，作者沿用yolov1的做法，即预测边界框中心点相对于对应栅格左上角位置的偏移量，同时使用sigmoid函数将边界框中心约束在当前栅格中：

$$
\begin{aligned}
b_x & = \sigma(t_x)+c_x \\
b_y & = \sigma(t_y)+c_y \\
b_w & = p_we^{t_w} \\
b_h & = p_he^{t_h} \\
Pr(object) * IOU(b, object) & = \sigma(t_o)
\end{aligned}
$$

附上论文解说图：

![yolov2 anchor boxes](/img/in_posts/20200110/yolov2_boxes.png)

其中，$p_h$和$p_w$表示经由k均值聚类产生的边界框的尺寸

所以，作者使用尺寸聚类以及直接预测边界框中心位置来生成边界框。

为了让模型能够较好地处理大小目标的检测问题，作者引入了细粒度特征（Fine-Grained Features）。模型不再像yolov1那样使用仅最后一特征层$13\times 13$来检测目标，而是加入了前面的特征层（$26\times 26$）的信息，由此引出了**passthrough层**。作者在后期代码中借鉴了ResNet，不直接对高分辨率的图像处理，而是先经过$1\times 1\times 64$卷积处理。实现passthrough层的逻辑大致是选取底层的特征图$26\times 26\times 512$，利用$1\times 1\times 64$进行降维为$26\times 26\times 64$，再拆分为4份堆叠通道数：$13\times 13\times 256$，与最后一层$13\times 13\times 1024$结合有：$13\times 13\times (1024+256) = 13\times 13\times 1280$。这就是passthrough的大致处理过程。特征信息经过passthrough层后，再经$3\times 3$和$1\times 1$处理，输出为$13\times 13\times 125$。

为了让模型更具鲁棒性，作者还采用了多尺度训练。每训练一定量的批次，就在尺寸范围了随机选取一个尺寸作为训练图像的输入尺寸（论文中选取10批次做一次变换）。因为模型为32倍的下采样，所以输入的图像尺寸应该为32的倍数，例如$\{320, 352, ..., 608\}$。

## Faster

通常的目标检测模型使用的特征提取网络是基于VGG-16的，这是一个十分强大但又有点庞大的网络。在先前的yolov1中，作者使用的特征提取网络是基于Googlenet架构的，运行速度要块于VGG-16，但它的精度是要低于VGG-16的。为了尽可能地同步提高模型运行效率和性能，作者重新设计了yolov2。

于是乎**Darknet-19**诞生了。该模型与VGG-16相似，主要使用了$3\times 3$卷积核并在池化操作后将通道数加倍。根据NIN（Network in Network）的结论，作者使用了全局平均池化（global average pooling），同时穿插使用$1\times 1$卷积核来压缩特征通道数。BN层自然是必不可少的，BN层不仅能加速模型收敛，还能正则化模型。最终，Darknet-19有19个卷积层、5个最大池化层，具体结构见下图：

![darknet-19](/img/in_posts/20200110/darknet_19.png)

特征提取网络搭建好后就是对它进行训练了，作者通过初始学习率设定为$0.1$的SGD梯度下降法在标准的ImageNet 1000类分类数据集进行了$160$次的迭代训练。对于学习率的更新问题，作者采用的是多项式衰减策略来更新学习率：

$$
lr_{cur} = (lr_{begin}-lr_{end})\cdot (1-\frac{gstep}{dstep})^{power} + lr_{end}
$$

其中，$lr_{begin}$为起始学习率，$lr_{end}$为最终学习率，$dstep$为衰减步数（decay steps），$gstep$为全局步数（global steps），并且有下式成立：

$$
gstep = \min(gstep, dstep)
$$

具体的参数设计请看作者原文。

在经过分辨率为$224\times 224$的初始训练后，再将输入图像分辨率提高到$448\times 448$对模型进行微调以适应大分辨率的图像。

Darknet-15训练结束后，作者将其最后一层卷积层去除并添加了3层带$3\times 3\times 1024$的卷积层，并在每一层后面加一个$1\times 1$的卷积层，这一层的通道数取决于检测的类别数。对于前面提到的passthrough层，作者将它用在了最后的$3\times 3\times 512$层和倒数第二个卷积层中。

到这里YoLov2的部分就结束了，后面是YoLo9000的主场。

## Stronger

为了能检测尽可能多的目标，作者提出了分类数据集与检测数据集联合训练的方法。在模型训练阶段，作者混合了来自两个数据集的图片，当遇到用于分类的图片时则只用网络结构中的分类部分来后向传播损失，当遇到用于检测的图像则基于整个损失函数进行后向传播。对于数据集的混合，面临的一个问题就是如何统一标签，而且大多数分类算法使用的损失函数为softmax函数，该函数假设了类之间为相互独立的，这并不符合这里的逻辑。

针对上述的问题，作者引入了等级分类，类似于生物学里的物种树。这里利用了Word Net（Imagenet数据集的标签就是取自于它的），它是基于认知的语言数据库，为一个有向图。作者并没有使用全部的图结构（毕竟这图十分庞大），而是仅用Imagenet中的概念来构建图结构。为了构建图，对比了Imagenet数据集的各个标签在WordNet中到根节点的路径，将具有唯一路径的词添加到了图中，对于有多个路径到达根节点的，则选择最短路径添加的图中，最终构建了一个$WordTree$。

在给定同义词集的情况下，针对该同义集的每个下位词的概率，预测每个节点的条件概率。例如在terrier节点上预测：

$$
Pr(Norfolk\ terrier\vert terrier) \\
Pr(Yorkshire\ terrier\vert terrier) \\
Pr(Bedlington\ terrier\vert terrier) \\
...
$$

若要计算特定节点的绝对概率，只需沿着特定节点到树的根节点的路径，乘以路径上节点的条件概率即可。例如Norfolk terrier节点：

$$
Pr(Norfolk\ terrier) = Pr(Norfolk\ terrier\vert terrier) \\
* Pr(terrier\vert hunting\ dog) \\
* ... \ * \\
* Pr(mammal\vert animal) \\
* Pr(animal\vert physical\ object)
$$

这里为了达到分类的目的令$Pr(physical\ object) = 1$，即图片必包含有目标。

在Imagenet 1000类的基础上构建WordTree来训练Darknet-19，添加所有的中间节点后，标签数量从1000扩展到1369。在训练中，将真实标签向上传递，例如一个标签为“Norfolk terrir”，它也应该被标为“dog”和“mammal”。为了计算条件概率，模型预测了一个1369长度的向量并计算所有相同概念下同义词集的softmax，如下图：

![WordTree1K](/img/in_posts/20200110/wordtree1k.png)

在分类时，作者假设了每幅图都包含一个对象，而对于检测任务，$Pr(physical\ object)$的值不再指定为1，而是由Yolov2的对象检测器给出该值。模型检测部分给出物体的一个边界框及其概率数，然后向下遍历WordTree，找到每个softmax分支的最高置信度路径直到到达某个阈值，此时路径上离根路径最远的节点即为预测标签。

在对Imagenet数据集和COCO数据集进行合并时，由于两个数据集的数量差异较大，所以作者对COCO数据集进行重采样，最后两个数据集的比例大致为$4:1$。模型沿用了YoLov2结构，但改为了使用3个先验框（原先5个先验框）来限制输出大小。分类时，模型仅后向传播分类损失部分，同时预测框于真实标签框的IOU不小于0.3时后向传播目标对象损失（objectness loss）；检测时，模型正常进行后向传播。

具体的细节可参考原论文

## 致谢

本文参考自以下：

>[YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)