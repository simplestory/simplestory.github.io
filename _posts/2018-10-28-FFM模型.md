---
layout:     post
title:      FFM模型原理
subtitle:   
date:       2018-10-28
author:     Simplestory
header-img: img/post-bg-debug.png
catalog: False
tags:
    - Machine Learning
---

>CTR(点击率)预测在内容推荐投放中有着重要地位，而FFM模型擅长于处理这方面的数据，FFM模型是FM模型的变种，或者说FM模型是FFM模型的特例

## 大致流程

1. 转换数据集格式（采用LIBSVM或FFMs式）
2. 计算损失函数（采用指数损失函数）
3. 优化参数（采用AdaGrad+SG策略）

## 转换数据格式

LIBSVM格式（不储存0值特征）：

$$label \quad feat1:val1 \quad feat2:val2 \quad ... \quad$$

FFMs格式：

$$label \quad field1:feat1:val1 \quad field2:feat2:val2 \quad ... \quad$$

针对不同的数据类型，数据格式的转换会有所不同

**针对类别特征(Categorical Features)**

例如有如下样本：

$$Yes \quad P:ESPN \quad A:Nike \quad G:Male$$

转为LIBSVM格式：

$$Yes \quad P-ESPN:1 \quad A-Nike:1 \quad G-Male:1$$

转为FFMs格式：

$$Yes \quad P:P-ESPN:1 \quad A:A-Nike:1 \quad G:G-Male:1$$

**数字型特征(Numerical Features)**

数字型特征有两种转换格式：第一种是将每一个特征视为一个域（这种方法只是在重复数据，没有增加信息）；第二种是离散化数据，并作为分类特征进行格式转换

例如：

Accepted |AR |Hidx |Cite
--------|----|-----|-----
Yes |45.73 |2  |3
No | 1.04|100 |50,000

第一格式：

$$Yes \quad AR:AR:45.73 \quad Hidx:Hidx:2 \quad Cite:Cite:3$$

第二格式：

$$Yes \quad AR:45:1 \quad Hidx:2:1 \quad Cite:3:1$$

对于第二格式，使用离散化后可能会丢失一些信息

**单域特征(Single-field Features)**

这种特征会让FFM模型降为FM模型

## 计算损失函数

在CTR预测中，常用的最优化为：

$$
min_{\mathbf{w}} \quad \frac{\lambda}{2}||\mathbf{w}||^2_2+\sum_{i=1}^m log(1+exp(-y_i\phi(\mathbf{w},\mathbf{x}_i)))
$$

其中函数$\phi(\mathbf{w},\mathbf{x}_i)$依据不同的模型会更换为不同的函数，针对FFM模型有：

$$
\phi_{FFM}(\mathbf{w},\mathbf{x}_i)=\sum_{j_i=1}^n\sum_{j_2=j_1+1}^n (\mathbf{w}_{j1,f2}\cdot\mathbf{w}_{j2,f1})x_{j1}x_{j2}
$$

其中$f_1$, $f_2$分别代表$j_1$, $j_2$的域

## 优化参数

参数的优化采用SG+AdaGrad的方法

首先计算梯度：

$$
\begin{aligned}
\mathbf{g}_{j1,f2} \equiv \nabla_{\mathbf{w}_{j1,f2}}f(\mathbf{w}) = \lambda \cdot \mathbf{w}_{j1,f2}+\kappa\cdot\mathbf{w}_{j2,f1}x_{j1}x_{j2}  \\
\mathbf{g}_{j2,f1} \equiv \nabla_{\mathbf{w}_{j2,f1}}f(\mathbf{w}) = \lambda \cdot \mathbf{w}_{j2,f1}+\kappa\cdot\mathbf{w}_{j1,f2}x_{j1}x_{j2}
\end{aligned}
\tag{1}
$$

其中：

$$
\kappa = \frac{\partial{log(1+exp(-y\phi_{FFM}(\mathbf{w},\mathbf{x})))}}{\partial{\phi_{FFM}({\mathbf{w},\mathbf{x}}})}
=\frac{-y}{1+exp(y\phi_{FFM}({\mathbf{w},\mathbf{x}}))}
$$

然后，借助AdaGrad算法累积梯度的平方有：

$$
\begin{aligned}
(G_{j1,j2})_d \leftarrow (G_{j1,f2})_d+(g_{j1,f2})_d^2  \\
(G_{j2,f1})_d \leftarrow (G_{j2,f1})_d+(g_{j2,f1})_d^2
\end{aligned}
\tag{2}
$$

其中d要遍历1到k（k为超参数，代表潜在向量的维度）

最后由下式更新相关参数：

$$
\begin{aligned}
(w_{j1,f2})_d \leftarrow (w_{j1,f2})_d-\frac{\eta}{\sqrt{(G_{j1,f2})_d}}(g_{j1,f2})_d  \\
(w_{j2,f1})_d \leftarrow (w_{j2,f1})_d-\frac{\eta}{\sqrt{(G_{j2,f1})_d}}(g_{j2,f1})_d
\end{aligned}
\tag{3}
$$

其中$\eta$为超参数——学习率，$\mathbf{w}$的初始值从服从在$[0,1/\sqrt{k}]$区间内均匀分布的分布中随即抽取，G的初值设为1，为防止$(G_{j1,f2})_d^{-\frac{1}{2}}$的值过大

实验显示，将每个样本统一到一致的长度会提高模型的训练正确率，同时，该模型对参数较为敏感

## 过拟合策略

FFM模型采用早停来防止过拟合，策略如下：

1. 将数据集划分为训练集和验证集
2. 在每轮训练之后，使用验证集计算模型的损失
3. 如果损失增大，记录训练轮数停止训练并转到步骤4
4. 如果需要，使用全部数据集重新训练模型，训练轮数为步骤3中所得的轮数

## 一些总结

与FM模型相比，FFM模型引入了域(field)的概念，增加了潜在向量的个数，如果有n个features属于f个field，则FFM模型的二次项有nf个隐向量，而FM可视为FFM的特例，是将所有特征都归属于一个field时的FFM模型

1. FFM模型对于包含有类别特征并转为二分类的数据有较好的表现
2. 若转换后的数据集不够稀疏，FFM模型可能不会有太好的表现
3. FFM模型难以处理数字型特征

该模型有三个超参数：

- k 潜在向量维度，实验表明其大小对模型并无太大影响
- $\lambda$ 正则化系数，其值过大会出现欠拟合，而过小易过拟合
- $\eta$ 学习率，过大易过拟合，过小则模型训练是误差下降缓慢

相关的库或框架：

[libffm](https://github.com/guestwalk/libffm)

[xlearn](https://github.com/aksnzhy/xlearn)

**参考论文**

[Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)