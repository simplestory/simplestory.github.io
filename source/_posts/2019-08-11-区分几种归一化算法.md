---
layout: post
title: 区分几种归一化算法
date: 2019-08-11
author: Simplestory
toc: True
mathjax: true
categories: 2019
tags: deep learning
---

> 神经网络已经有几个归一化算法了，它们长得都很相似，基本上都有统一的形式。本文参考了知乎专栏SIGAI关于归一化算法的文章，具体链接在文章底部。

## 基本问题

在使用神经网络对数据集进行学习时，由于数据集的来源可能有着较大的差异，数据集不同属性的取值有时也会有很大的范围，这些都会引起数据的取舍误差，导致神经网络的学习效果不佳。为了解决这个问题，我们对数据进行归一化处理，即将有着角的取值方位的数据规范到同一尺度下，这样处理有利于网络的学习。发展到现在，归一化有了许多变种算法，常见的有：Batch Normalization(BN)、Layer Normalization(LN)、Instance Normalization(IN)以及Group Normalization(GN)，其实它们的处理方式都是类似的（减去均值，除以标准差，再进行线性映射），区别就在于计算时的纬度不一样，具体公式如下：

$$
y = \gamma \left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta
$$

注意这里还有一点不同，对于$BN$、$IN$和$GN$，参数$\gamma$、$\beta$的纬度都等于通道数$C$的向量，对于$LN$，这两个参数都是纬度等于$normalized\ shape$的矩阵。

**为了后面说明方便，这里预先定义数据集纬度为：$x \in R^{N \times C \times H \times W}$，具体含义是数据集包含$N$个样本，每个样本通道数为$C$，高为$H$，宽为$W$。**

## Batch Normalization

该方法是最早的归一化算法，效果也是不错的，可以说是最好的。计算纬度是$N \times H \times W$，保留通道数。具体公式如下：

$$
\begin{aligned}
    \mu_c(x) & = \frac{1}{NHW}\sum_{n=1}^N\sum_{h=1}^H\sum_{w=1}^W x_{nchw}  \\
    \sigma_c(x) & = \sqrt{\frac{1}{NHW}\sum_{n=1}^N\sum_{h=1}^H\sum_{w=1}^W (x_{nchw}-\mu_c(x))^2+\epsilon}
\end{aligned}
$$

即保留通道数，将第一个样本的第一个通道加上第二个样本的第一个通道这样接下去知道第$N$个样本的第一个通道，在除以$N\times H \times W$得到一个数就是第一通道的平均数（而不是一个$H\times W$的矩阵），求标准差时也是一样的。

$BN$有一个缺点，就是需要大量的数据才能合理地估计出训练数据的均值和方差，这很有可能会导致内存不够，而且对于$RNN$模型长短不一的训练数据长度，$BN$也很难应用。

**这里注意一点，当在网络中采用了BN层时，就不必要再使用Dropout层了**

## Layer Normalization

$LN$有一个很大的优势，就是它不需要批训练，针对单个样本数据就能进行归一化。计算纬度是$C \times H \times W$，保留样本数。具体公式如下：

$$
\begin{aligned}
    \mu_n(x) & = \frac{1}{CHW}\sum_{c=1}^C\sum_{h=1}^H\sum_{w=1}^W x_{nchw}  \\
    \sigma_n(x) & = \sqrt{\frac{1}{CHW}\sum_{c=1}^C\sum_{h=1}^H\sum_{w=1}^W (x_{nchw}-\mu_n(x))^2+\epsilon}
\end{aligned}
$$

即保留样本数，将样本的通道数、高和宽对应的数据相加再除以$C \times H \times W$得到的即为该样本的平均值，标准差同理。

## Instance Normalization

在生成模型中，数据集中各个通道数的均值和标准差会影响到最终生成图片的风格，所以$IN$的作者认为可以先把图像在通道层面上进行归一化，然后再用目标风格图片的均值和标准差去归一化，这样来获得目标图片的风格。$IN$也不需要批训练，针对单个样本就能归一化。计算纬度是$H \times W$，保留样本数和通道数。具体公式如下：

$$
\begin{aligned}
    \mu_{nc}(x) & = \frac{1}{HW}\sum_{h=1}^H\sum_{w=1}^W x_{nchw}  \\
    \sigma_{nc}(x) & = \sqrt{\frac{1}{HW}\sum_{h}^H\sum_{w=1}^W (x_{nchw}-\mu_{nc}(x))^2+\epsilon}
\end{aligned}
$$

即保留样本数和通道数，对单一样本的单个通道求高和宽对应数据的和再除以$H \times W$即为所求均值。

## Group Normalization

$GN$主要应用于像图像分割这类占用显存较大的学习任务。这类任务由于受到显存大小的限制，所以设置的$batchsize$通常为个位数，$BN$根本没能发挥效果。$GN$是$LN$和$IN$的折中，也是对单一样本可以进行归一化。它将通道分为$G$组，每组有$C/G$个通道，然后再对各组中的元素求均值和标准差，并独立地用所求得的参数对对应组的通道进行归一化。具体计算公式如下：

$$
\begin{aligned}
    \mu_{ng}(x) & = \frac{1}{(C/G)HW}\sum_{c=gC/G}^{(g+1)C/G}\sum_{h=1}^H\sum_{w=1}^W x_{nchw}  \\
    \sigma_{ng}(x) & = \sqrt{\frac{1}{(C/G)HW}\sum_{c=gC/G}^{(g+1)C/G}\sum_{h=1}^H\sum_{w=1}^W (x_{nchw}-\mu_{ng}(x))^2+\epsilon}
\end{aligned}
$$

## 致谢

本文大部分参考自下面这篇文章：

>[知乎专栏SIGAI（作者：伊相楠）](https://zhuanlan.zhihu.com/p/69659844)